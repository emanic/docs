// WE PULL THIS CONTENT FROM https://github.com/aporeto-inc/junon
// DO NOT EDIT THIS FILE.
// YOU MUST SUBMIT A PR AGAINST THE UPSTREAM REPO.
// THE UPSTREAM REPO IS CURRENTLY PRIVATE.

== Deploy Segment Console across multiple nodes

=== Overview

Running Segment Console in a single container makes it easy to get
started. After some time, you may choose to move to a multi-container
deployment for one or more of the following reasons.

* Better performance at scale
* Kubernetes
https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/[pod]
and
https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/#cluster-autoscaling[cluster]
auto scaling
* Resource metrics via https://prometheus.io/[Prometheus] and
https://grafana.com/[Grafana]
* Tracing and debugging information via
https://www.jaegertracing.io/[Jaeger]

You can migrate the data and keep the services up during the process.

We strongly recommend using the same Kubernetes cluster for the single
container and multiple container Segment Consoles. Otherwise, the data
migration may fail.

=== About Voila

Voila is a toolbox container that allows you to:

* Manage a Voila environment that wraps all your resources and targets a
specific Kubernetes cluster.
* Prepares all the values and certificates that will be injected in the
Helm charts to deploy Segment Console.
* Secure your certificates and other secrets.
* Once activated, wraps Kubernetes and Helm commands to target your
Kubernetes cluster.

Your Voila environment contains the following:

* `activate` script: activates your Voila environment
* `aporeto.yaml` file: contains all the settings used for the deployment
* `certs` folder: contains all generated certificates.
* `conf.d` folder: contains service configurations
* `conf.voila` file: contains the Voila settings

By default, `aporeto.yaml` and all certificates are encrypted when the
environment is not activated.

=== Before you begin

==== Domain name

We recommend fronting Segment Console with a load balancer and having a
DNS record pointing to it.

You can use a single URL for the web interface and API. For example,
`+https://segment.acme.com+`. The API uses a different port, `4443` by
default.

You can also use a different URL for each one. For example:

* `+https://ui.segment.acme.com+`
* `+https://api.segment.acme.com+`

Note that most Kubernetes providers assign service IPs and URLs only at
the time of deployment. You will then need to make the link between the
URL above and what is assigned. Voila will help you with that process.

==== Using custom certificates

By default, Segment Console uses a certificate signed by a certificate
authority (CA) specific to your cluster. While the certificates are
valid, they wonâ€™t be trusted by browsers and other clients. If you wish
to supply Segment Console with custom certificates, complete the
following preliminary steps.

Given the following Segment Console domain names:

* User interface accessible from `segment.acme.com`
* API accessible from `api.segment.acme.com`
* (optional) Monitoring accessible from `monitor.segment.acme.com`
* (optional) Tracing accessible from `tracing.segment.acme.com`

Request an X.509 certificate signed by a trusted CA with the following
information:

* Common name (CN): `segment.acme.com`
* Subject alternative names (SAN): `api.segment.acme.com`,
`monitoring.segment.acme.com`, `tracing.segment.acme.com`

Once done you should have the following certificates in X.509 format:

* A CA chain with intermediate chain if any named `ca_int.pem`
* A certificate signed by the above CA named `cert.pem`
* A private key *not protected by passphrase* named `key.pem`

=== Requirements

==== Voila host

* Cannot be a node that will host Segment Console itself
* Workstation, bastion instance, or jump box
* Equipped with https://docs.docker.com/get-docker/[Docker 18.02 or
later].

==== Kubernetes cluster

* (GKE only) `container.clusterRoles.create` permission
* Must adhere to
https://github.com/cncf/k8s-conformance/blob/master/instructions.md[conformance
testing]
* Minimum sizing as follows.
+
[width="99%",cols="<19%,<18%,<17%,<21%,<9%,<16%",options="header",]
|===
|Label |Description |Specs |GCP |AWS |Min. nodes
|`type=mongodb` |Mongo database |16 vCPU / 64GB RAM |n2-standard-16
(COS) |m5a.4xlarge |3

|`type=influxdb` |Influx database |16 vCPU / 64GB RAM |n2-standard-16
(COS) |m5a.4xlarge |1 (standalone)

|`type=service` |Aporeto services |16 vCPU / 32GB RAM |e2-standard-16
(COS) |c5.4xlarge |3

|`type=monitoring` |Aporeto monitoring |8 vCPU / 16GB RAM |e2-standard-8
(COS) |c5.2xlarge |2

|`type=highwind` |Aporeto apps |4 vCPU / 16GB RAM |n1-standard-4 (COS)
|m4.xlarge |2
|===

=== Configure kubectl

Set up your `kubeconfig` to point to the target cluster. In this
procedure, we assume that the cluster contains the single container
Segment Console and is also the destination for the multi-container
Segment Console.

[arabic]
. Retrieve the cluster credentials. A couple of examples follow.
* GKE
+
[source,console]
----
gcloud container clusters get-credentials <cluster-name> --zone <region> --project <project>
----
* EKS
+
[source,console]
----
aws eks update-kubeconfig --region <region> --name <cluster-name>
----
. Check your `kubectl` context points to the correct Kubernetes cluster.
+
[source,console]
----
kubectl get nodes
----
. Generate the tarball file from the single container Segment Console as
follows.
+
[source,console]
----
kubectl -n <namespace> exec console-0 generate-voila-seed > aporeto/voila.seed.tgz
----

=== Set up Voila

[arabic]
. From your Voila host, create a folder that will hold the Voila
environment.
+
[source,console]
----
mkdir segment
----
. Navigate into the directory.
+
[source,console]
----
cd segment
----
. From your Voila host, pull the Voila container.
+
[source,console]
----
docker pull gcr.io/aporetodev/voila:release-3.15.0
----

=== Start the Voila installer

[arabic]
. Start the interactive install.
+
[source,console]
----
docker run -ti \
  -v $PWD:/voila-env \
  -v ~/.kube:/root/.kube \
  docker.io/aporeto/voila:release-3.15.0 \
  create
----
. You will see a welcome message.
. Provide the domain name or IP address that you want to use for your
Segment API and web interface.
. Skip the monitoring and backups for now. You can add these later.
. Provide a name for your deployment.
. It should identify the correct Kubernetes context, you can press
ENTER.
. At the prompt inquiring how to expose the Segment Console services, we
recommend typing `2`.
+
[source,console]
----
How the public facing services will be exposed?
1) node port
2) load balancer
3) ingress controller
#? 2
----
. At the prompt that asks if you want to create your deployment, if you
are not using custom certificates, type `yes`. If you are using custom
certificates, type `no`.
+
[source,console]
----
Your voila environment is now created.
If you are satisfied with this configuration, you can deploy now.
Otherwise a dry run will be performed and a summary displayed.

Do you want to start the deployment? (y/n) default is yes: y
----
. A message will be displayed informing you that it detects a seed to
use. The seed will be destroyed after the installation process. The
final `apostate` step will fail if you are using the same URL. You can
disregard this for now.
+
[source,console]
----
Importing settings from voila seed file /voila-env/aporeto/voila.seed.tgz
----
. Save the secret key in a safe place as directed. The Voila environment
contains all the secrets and certificates used by Segment Console. It is
protected by a generated key that you must keep in a safe place. Without
this key you will *not* be able to load the environment again. This key
is only printed *once* at the end of the Voila environment creation
process.
+
[source,console]
----
!!! Please remember to save the following shared key in a safe place. !!!
!!! Don't lose it, or your voila environment will be locked forever.  !!!
Key: AEdJVENSWVBUS0VZAAAAAgAAAAAAAAABAAAABAAAAAAAAAADAAAAIBImyzKjwCMSrjsuy6XbP8u59s5VLYZyWbjcO2xcyZ74AAAABQAAAEBjZYGZzorYp9MeOyr9dz/wXSRNYkyw8fe0rlfreUQXqOY7PS3vsmB54G6zlhqNkB0odlGTAVhWVwDyZ5Z6TslwAAAAAA==
----

=== Activate Voila

[arabic]
. Activate Voila.
+
[source,console]
----
cd segment && ./activate
----
. Provide the key at the prompt.

=== Add custom certificates (optional)

If you wish to use custom certificates, complete the following steps.
Otherwise, skip to link:#migrate-your-data[Migrate your data].

From within your activated Voila environment:

[arabic]
. Create a `/certs/public-ca.pem` file as:
+
[source,console]
----
mkdir -p /certs
cat ca_int.pem > /certs/public-ca.pem
----
. Create a `/certs/public-cert.pem` file by concatenating the
certificate and the chain. The order matters. The final
`public-cert.pem` certificate must present the server certificate before
the CA.
+
[source,console]
----
cat cert.pem ca_int.pem > /certs/public-cert.pem
----
. Create a `/certs/public-key.pem` file that will contain the private
key:
+
[source,console]
----
cat key.pem > /certs/public-key.pem
----
. Then run
+
[source,console]
----
doit -u aporeto-backend clad --force
----
. Return to link:#start-the-voila-installer[Start the Voila installer]
and repeat the steps. However, modify the first command as below Note
the addition of `-v $PWD/certs:/certs`. That will instruct Voila to use
your CA instead of the Segment-generated one.
+
[source,console]
----
docker run -ti \
  -v $PWD:/voila-env \
  -v $PWD/certs:/certs \
  -v ~/.kube:/root/.kube \
  docker.io/aporeto/voila:release-3.15.0 \
  create
----

After you complete the steps, Segment Console should be using your
provided certificate.

The following files:

* `/certs/public-ca.pem`
* `/certs/public-cert.pem`
* `/certs/public-key.pem`

Will be securely integrated into the main configuration and will be
deleted in the process.

=== Migrate your data

You can migrate your databases by streaming the data across the network.

==== MongoDB

From your activated Voila environment:

[arabic]
. Launch the restoration tool by issuing the `restore` command.
. Select `1) mongodb` at the prompt.
. Select `5) Listen for data stream` at the prompt. It will display
something like:
+
[source,console]
----
Listening for data stream:
10.80.2.15:42000
Use the above line as streamer target
----
. Start the migration. It might take a bit of time especially if you had
a lot of activity logs.
+
[source,console]
----
kubectl -n <namespace> exec console-0 mongodb-stream to 10.80.2.17:42000
----
. Once done you can exit the restore tool by pressing CTRL+D.

==== InfluxDB

From your activated Voila environment:

[arabic]
. Launch the restoration tool by issuing the `restore` command.
. Select `2) influxdb` at the prompt.
. Select `5) Listen for data stream` at the prompt. It will display
something like:
+
[source,console]
----
Listening for data stream on:
https://10.84.8.59:8086 admin Baisaeng3Ap7ru5uo4aeze6duexedae3yie6chooyiar7uz2aechieQu1ogei5ko 90
Use the above line as streamer target. The last parameter is the number of days to synchronize (90 by defaults).
----
. Start the migration. If you donâ€™t want to keep 90 days of data you can
change the last number in the parameters. For example, set it to `2` to
get only the last two days of data. It might take a bit of time
especially if you had a lot of flows.
+
[source,console]
----
kubectl -n <namespace> exec console-0 influxdb-stream to https://10.84.8.59:8086 admin Baisaeng3Ap7ru5uo4aeze6duexedae3yie6chooyiar7uz2aechieQu1ogei5ko 90
----
. Once done you can exit the restore tool by pressing CTRL+D.

==== Switch the URL to the multi-container deployment

At this point data are synced between the single container and the
multi-container deployment. All we need to do now is to make sure that
the URLs are pointing to the correct location.

From your activated Voila environment:

* Ensure that `get_value global.public.api` is pointing to the service
`k get svc wutai`
* Ensure that `get_value global.public.ui` is poiting to the service
`k get svc clad`

For instance if you were using a load balancer just update the DNS
record, and for its propagation.

Once done, you should correctly see your workloads move to the new
multi-container setup.

=== Clean up the single container

Issue the following command to remove the single container Segment
Console.

[source,console]
----
helm -n <namespace> uninstall console
----

You may have to clean the storage by hand.

[source,console]
----
kubectl -n <namespace> delete pvc data-console-0 backup-console-0
----

=== Additional configuration options

==== Proxy

If you need to go through an identity provider proxy, you will need to
configure the following.

[source,console]
----
set_value global.proxy.enabled true
set_value global.proxy.http http://proxy:port
set_value global.proxy.https https://proxy:port
----

==== SMTP

Run the following commands to configure SMTP. User and password are
optional.

[source,shell]
----
set_value integrations.smtp.enabled "true"
set_value integrations.smtp.server "smtp.gmail.com:587"
set_value integrations.smtp.user "user@domain.com"
set_value integrations.smtp.pass "secret"
set_value integrations.smtp.systemEmail "aporeto@domain.com"
set_value global.integrations.smtp.receivers.invitation "registration@domain.com"
set_value global.integrations.smtp.receivers.monitor "alerts@domain.com"
set_value immediateActivation false vince override
----
